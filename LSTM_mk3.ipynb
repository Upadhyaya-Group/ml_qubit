{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "\r\n",
    "\r\n",
    "import tensorflow as tf\r\n",
    "import random\r\n",
    "from tensorflow.keras.models import Sequential\r\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "from matplotlib import pyplot\r\n",
    "import data_generator\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# load in the data \r\n",
    "\r\n",
    "full_dataset = np.load(\"ml_dataset.npy\");"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# split up the data into the x and y halves\r\n",
    "\r\n",
    "x_set = full_dataset[0];\r\n",
    "y_set = full_dataset[1];"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "# split the data into training and testing sets\r\n",
    "\r\n",
    "\r\n",
    "half = int(3*len(x_set)/4);\r\n",
    "\r\n",
    "print(f'half: {half}');\r\n",
    "x_train = x_set[0:half];\r\n",
    "x_test = x_set[half:];\r\n",
    "\r\n",
    "y_train = y_set[0:half];\r\n",
    "y_test = y_set[half:];\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "half: 74\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "# print some data diagnostics to ensure we correctly obtained data sets\r\n",
    "\r\n",
    "print(f\"x_set shape: {x_set.shape}\");\r\n",
    "print(f\"y_set shape: {y_set.shape}\");\r\n",
    "\r\n",
    "\r\n",
    "training_point_number = x_set.shape[1];\r\n",
    "print(f\"number of points in each graph set: {training_point_number}\");\r\n",
    "\r\n",
    "\r\n",
    "print(\"number of x training data sets:\");\r\n",
    "print(len(x_train[0:]));\r\n",
    "\r\n",
    "\r\n",
    "print(\"number of y training data sets:\");\r\n",
    "print(len(y_train[0:]));\r\n",
    "\r\n",
    "assert(len(x_train) == len(y_train));\r\n",
    "assert(len(x_test) == len(y_test));\r\n",
    "assert(x_set.shape == y_set.shape);\r\n",
    "\r\n",
    "print(f\"\\nNumber of training graph-solution sets: {len(x_train)}\")\r\n",
    "\r\n",
    "\r\n",
    "print(f\"Number of testing graph-solution sets: {len(x_test)}\")\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "print(f\"\\nDataset shape: {x_set.shape}\")\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x_set shape: (99, 500)\n",
      "y_set shape: (99, 500)\n",
      "number of points in each graph set: 500\n",
      "number of x training data sets:\n",
      "74\n",
      "number of y training data sets:\n",
      "74\n",
      "\n",
      "Number of training graph-solution sets: 74\n",
      "Number of testing graph-solution sets: 25\n",
      "\n",
      "Dataset shape: (99, 500)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "# reshape the x data into a column of 1 element rows for the LSTM\r\n",
    "\r\n",
    "\r\n",
    "x_train = x_train.reshape(x_train.shape[0],training_point_number,1);\r\n",
    "x_test = x_test.reshape(x_test.shape[0],training_point_number,1);\r\n",
    "\r\n",
    "print(x_train.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(74, 500, 1)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "#build the model by stacking layers\r\n",
    "\r\n",
    "\r\n",
    "model = tf.keras.models.Sequential()\r\n",
    "\r\n",
    "model.add(LSTM(training_point_number));\r\n",
    "model.add(Dense(training_point_number,activation='relu'));\r\n",
    "model.add(Dropout(0.2));\r\n",
    "\r\n",
    "model.add(Dense(training_point_number,activation='relu'));\r\n",
    "model.add(Dropout(0.2));\r\n",
    "\r\n",
    "model.add(Dense(training_point_number,activation='relu'));\r\n",
    "model.add(Dropout(0.2));\r\n",
    "\r\n",
    "model.add(Dense(training_point_number,activation='relu'));\r\n",
    "model.add(Dropout(0.2));\r\n",
    "\r\n",
    "model.add(Dense(training_point_number,activation='relu'));\r\n",
    "model.add(Dropout(0.2));\r\n",
    "\r\n",
    "model.add(Dense(training_point_number,activation='softmax'));\r\n",
    "\r\n",
    "\r\n",
    "print(\"model built\");\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "model built\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "# get the predictions for the second datapoint\r\n",
    "predictions = model(x_train)\r\n",
    "\r\n",
    "# print the raw predictions for that datapoint. Note that these are \"logits\" or \"log-odds\" scores, which is related to the probability, \r\n",
    "# and also is the inverse of the sigmoid function\r\n",
    "print(\"logits prediction: \");\r\n",
    "print(predictions[0]);\r\n",
    "\r\n",
    "# we can use the softmax function to convert these logits to probabilities for each preciction class\r\n",
    "print(\"odds prediction: \");\r\n",
    "print(tf.nn.softmax(predictions).numpy()[0]);\r\n",
    "\r\n",
    "\r\n",
    "print(f'shape: {predictions.shape}');\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "logits prediction: \n",
      "tf.Tensor(\n",
      "[0.00200001 0.00200001 0.00199998 0.00200008 0.00200003 0.00199999\n",
      " 0.00200006 0.00200005 0.00200005 0.00199992 0.00200001 0.00199999\n",
      " 0.00200006 0.00200011 0.00200001 0.00200001 0.00199998 0.002\n",
      " 0.00200006 0.00200001 0.00199995 0.00199999 0.00200005 0.00200004\n",
      " 0.00200001 0.00199997 0.00199996 0.00199993 0.00200002 0.00200006\n",
      " 0.00199998 0.00199998 0.00199982 0.00199998 0.00200006 0.00199991\n",
      " 0.00199999 0.00200007 0.00200011 0.00199998 0.00199989 0.00199994\n",
      " 0.00200002 0.00200006 0.00199999 0.002      0.00199998 0.00200001\n",
      " 0.00200002 0.00200002 0.00199993 0.00200008 0.0019999  0.00199999\n",
      " 0.00200002 0.00200001 0.002      0.00199997 0.00199996 0.00199992\n",
      " 0.00200004 0.00200004 0.00200002 0.00200004 0.00200003 0.00200002\n",
      " 0.00200003 0.00200003 0.0019999  0.00199991 0.00199995 0.00199997\n",
      " 0.00199998 0.00200007 0.00199995 0.00200011 0.00200004 0.00200003\n",
      " 0.00200003 0.00199996 0.00199998 0.00200001 0.00200006 0.00199991\n",
      " 0.00200004 0.00200003 0.00200014 0.00199991 0.00200006 0.00200009\n",
      " 0.00199997 0.00200008 0.00200003 0.00200005 0.00199994 0.00199993\n",
      " 0.00200002 0.00199995 0.00200011 0.00200012 0.00199995 0.002\n",
      " 0.002      0.00200004 0.00199989 0.00200011 0.00200011 0.00199996\n",
      " 0.00199996 0.00199997 0.00200007 0.00200013 0.00200006 0.00199992\n",
      " 0.00199996 0.00200009 0.00199988 0.00199999 0.00200001 0.00199999\n",
      " 0.00199994 0.00199999 0.00199998 0.00199985 0.00200003 0.00199999\n",
      " 0.00199996 0.00200002 0.00200001 0.00200001 0.00199997 0.00200003\n",
      " 0.00200005 0.00199995 0.00199987 0.00199995 0.00199996 0.00199993\n",
      " 0.002      0.00199996 0.00200009 0.00199993 0.00200003 0.00200004\n",
      " 0.00200001 0.00200003 0.00199985 0.00200006 0.00199986 0.00200003\n",
      " 0.00199997 0.00199987 0.00199996 0.00200002 0.00199998 0.00200005\n",
      " 0.00199995 0.00200002 0.00200009 0.0020001  0.00200001 0.00200013\n",
      " 0.00199999 0.00199999 0.00200012 0.00199996 0.00199986 0.00199993\n",
      " 0.00200016 0.00199997 0.00199987 0.0020001  0.00200013 0.00199999\n",
      " 0.00200007 0.00200009 0.00199994 0.00200003 0.00200004 0.00200001\n",
      " 0.00200007 0.00200007 0.00199998 0.00199987 0.00199997 0.00200002\n",
      " 0.00199997 0.002      0.00199998 0.00200002 0.00200005 0.00199999\n",
      " 0.00200004 0.00199997 0.00199993 0.00199995 0.00200006 0.00200001\n",
      " 0.00199996 0.00199984 0.00199989 0.002      0.00199998 0.00200009\n",
      " 0.00200012 0.00200004 0.00199994 0.00200004 0.00199995 0.00200005\n",
      " 0.00200007 0.00199998 0.002      0.00199992 0.00200008 0.00200003\n",
      " 0.00199995 0.00200003 0.00199994 0.00199997 0.0019999  0.002\n",
      " 0.00200007 0.00199995 0.00199986 0.00199995 0.002      0.00200011\n",
      " 0.00199994 0.00200004 0.00199998 0.00199988 0.002      0.00200006\n",
      " 0.00199998 0.00200008 0.002      0.00199998 0.00199987 0.00199991\n",
      " 0.00200012 0.00200001 0.00200007 0.00200008 0.00200001 0.00200009\n",
      " 0.00200003 0.00200011 0.00200008 0.00200013 0.00200001 0.00199999\n",
      " 0.00200007 0.00199993 0.00200002 0.00200006 0.00199999 0.00200002\n",
      " 0.00199991 0.00199999 0.00199996 0.00199998 0.00200001 0.00199997\n",
      " 0.00199993 0.00199993 0.00199992 0.00200004 0.00199999 0.002\n",
      " 0.00200004 0.00199992 0.00200002 0.00200001 0.00199999 0.00200005\n",
      " 0.00199996 0.00199998 0.00200005 0.00200005 0.00199996 0.00199989\n",
      " 0.00200001 0.002      0.00200003 0.00199996 0.00200005 0.002\n",
      " 0.00199989 0.00199999 0.00200002 0.002      0.00200014 0.00200007\n",
      " 0.00199987 0.00200002 0.00200005 0.00199993 0.002      0.00200002\n",
      " 0.00200012 0.00200003 0.00199991 0.00199998 0.00199999 0.00200002\n",
      " 0.00199994 0.00200006 0.00200001 0.00200001 0.00200001 0.00199984\n",
      " 0.00200005 0.002      0.00199991 0.00200012 0.00200011 0.00200001\n",
      " 0.00200006 0.00200005 0.00199992 0.00199985 0.00200007 0.00200005\n",
      " 0.00199995 0.00200006 0.00199999 0.00199994 0.00200007 0.00199998\n",
      " 0.00199999 0.00199995 0.0019999  0.00200005 0.00199997 0.00199993\n",
      " 0.00199987 0.00200002 0.00200003 0.00199998 0.00200018 0.00199995\n",
      " 0.00200007 0.00199996 0.00199994 0.0020001  0.00199995 0.00200002\n",
      " 0.00199995 0.00200005 0.00199993 0.00199996 0.00200001 0.00200015\n",
      " 0.00200003 0.00200004 0.00199988 0.00199997 0.00199998 0.00199996\n",
      " 0.00199998 0.00200008 0.00200002 0.002      0.00200008 0.00200008\n",
      " 0.00200006 0.00199997 0.00199998 0.00199994 0.00200004 0.00199994\n",
      " 0.00200002 0.00199999 0.00199998 0.0020001  0.00199998 0.00199989\n",
      " 0.00199998 0.00200001 0.00199999 0.00200004 0.00200015 0.00200006\n",
      " 0.00200007 0.00200001 0.00200008 0.00200005 0.00200004 0.00200005\n",
      " 0.00199998 0.002      0.00199996 0.00200004 0.00200005 0.00199995\n",
      " 0.00200008 0.00199994 0.00200003 0.00199992 0.00200002 0.00200003\n",
      " 0.00199987 0.00199991 0.00200011 0.00199997 0.00200004 0.00200012\n",
      " 0.00200001 0.00200004 0.002      0.00199987 0.00200006 0.00200008\n",
      " 0.00199996 0.00199995 0.00200012 0.00199992 0.00199987 0.00200001\n",
      " 0.00199997 0.00199992 0.00200005 0.00199994 0.002      0.00200007\n",
      " 0.00199988 0.00199999 0.00199999 0.00200006 0.00199995 0.00199998\n",
      " 0.00199989 0.00200003 0.00200001 0.00199988 0.00200011 0.00200008\n",
      " 0.00200005 0.00200005 0.00199997 0.00200011 0.0019999  0.00199996\n",
      " 0.00199993 0.00199991 0.00199998 0.00199991 0.00200002 0.00199993\n",
      " 0.00199996 0.00199992 0.00199996 0.00199995 0.00200002 0.0020001\n",
      " 0.00200005 0.00200002 0.00200001 0.002      0.00200008 0.00200005\n",
      " 0.00199994 0.00200005 0.00199987 0.00199996 0.00199994 0.00200015\n",
      " 0.00200005 0.00199998 0.00199991 0.00199984 0.00200001 0.00199986\n",
      " 0.00199993 0.00199999 0.002      0.00200002 0.00200002 0.00199993\n",
      " 0.00199995 0.00199995 0.00199992 0.00199999 0.00200001 0.00200007\n",
      " 0.00200002 0.00199999 0.00199996 0.00200001 0.00199997 0.00200005\n",
      " 0.00200017 0.00199999 0.002      0.00200017 0.00200001 0.00200004\n",
      " 0.00200001 0.00199987], shape=(500,), dtype=float32)\n",
      "odds prediction: \n",
      "[0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002]\n",
      "shape: (74, 500)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "print(y_train[0])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[  106.81238782   128.00362308   149.19485833   170.38609359\n",
      "   191.57732885   212.76856411   233.95979937   255.15103463\n",
      "   276.34226988   297.53350514   318.7247404    339.91597566\n",
      "   361.10721092   382.29844618   403.48968144   424.68091669\n",
      "   445.87215195   467.06338721   488.25462247   509.44585773\n",
      "   530.63709299   551.82832824   573.0195635    594.21079876\n",
      "   615.40203402   636.59326928   657.78450454   678.97573979\n",
      "   700.16697505   721.35821031   742.54944557   763.74068083\n",
      "   784.93191609   806.12315135   827.3143866    848.50562186\n",
      "   869.69685712   890.88809238   912.07932764   933.2705629\n",
      "   954.46179815   975.65303341   996.84426867  1018.03550393\n",
      "  1039.22673919  1060.41797445  1081.6092097   1102.80044496\n",
      "  1123.99168022  1145.18291548  1166.37415074  1187.565386\n",
      "  1208.75662126  1229.94785651  1251.13909177  1272.33032703\n",
      "  1293.52156229  1314.71279755  1335.90403281  1357.09526806\n",
      "  1378.28650332  1399.47773858  1420.66897384  1441.8602091\n",
      "  1463.05144436  1484.24267961  1505.43391487  1526.62515013\n",
      "  1547.81638539  1569.00762065  1590.19885591  1611.39009116\n",
      "  1632.58132642  1653.77256168  1674.96379694  1696.1550322\n",
      "  1717.34626746  1738.53750272  1759.72873797  1780.91997323\n",
      "  1802.11120849  1823.30244375  1844.49367901  1865.68491427\n",
      "  1886.87614952  1908.06738478  1929.25862004  1950.4498553\n",
      "  1971.64109056  1992.83232582  2014.02356107  2035.21479633\n",
      "  2056.40603159  2077.59726685  2098.78850211  2119.97973737\n",
      "  2141.17097263  2162.36220788  2183.55344314  2204.7446784\n",
      "  2225.93591366  2247.12714892  2268.31838418  2289.50961943\n",
      "  2310.70085469  2331.89208995  2353.08332521  2374.27456047\n",
      "  2395.46579573  2416.65703098  2437.84826624  2459.0395015\n",
      "  2480.23073676  2501.42197202  2522.61320728  2543.80444254\n",
      "  2564.99567779  2586.18691305  2607.37814831  2628.56938357\n",
      "  2649.76061883  2670.95185409  2692.14308934  2713.3343246\n",
      "  2734.52555986  2755.71679512  2776.90803038  2798.09926564\n",
      "  2819.29050089  2840.48173615  2861.67297141  2882.86420667\n",
      "  2904.05544193  2925.24667719  2946.43791244  2967.6291477\n",
      "  2988.82038296  3010.01161822  3031.20285348  3052.39408874\n",
      "  3073.585324    3094.77655925  3115.96779451  3137.15902977\n",
      "  3158.35026503  3179.54150029  3200.73273555  3221.9239708\n",
      "  3243.11520606  3264.30644132  3285.49767658  3306.68891184\n",
      "  3327.8801471   3349.07138235  3370.26261761  3391.45385287\n",
      "  3412.64508813  3433.83632339  3455.02755865  3476.21879391\n",
      "  3497.41002916  3518.60126442  3539.79249968  3560.98373494\n",
      "  3582.1749702   3603.36620546  3624.55744071  3645.74867597\n",
      "  3666.93991123  3688.13114649  3709.32238175  3730.51361701\n",
      "  3751.70485226  3772.89608752  3794.08732278  3815.27855804\n",
      "  3836.4697933   3857.66102856  3878.85226382  3900.04349907\n",
      "  3921.23473433  3942.42596959  3963.61720485  3984.80844011\n",
      "  4005.99967537  4027.19091062  4048.38214588  4069.57338114\n",
      "  4090.7646164   4111.95585166  4133.14708692  4154.33832217\n",
      "  4175.52955743  4196.72079269  4217.91202795  4239.10326321\n",
      "  4260.29449847  4281.48573372  4302.67696898  4323.86820424\n",
      "  4345.0594395   4366.25067476  4387.44191002  4408.63314528\n",
      "  4429.82438053  4451.01561579  4472.20685105  4493.39808631\n",
      "  4514.58932157  4535.78055683  4556.97179208  4578.16302734\n",
      "  4599.3542626   4620.54549786  4641.73673312  4662.92796838\n",
      "  4684.11920363  4705.31043889  4726.50167415  4747.69290941\n",
      "  4768.88414467  4790.07537993  4811.26661519  4832.45785044\n",
      "  4853.6490857   4874.84032096  4896.03155622  4917.22279148\n",
      "  4938.41402674  4959.60526199  4980.79649725  5001.98773251\n",
      "  5023.17896777  5044.37020303  5065.56143829  5086.75267354\n",
      "  5107.9439088   5129.13514406  5150.32637932  5171.51761458\n",
      "  5192.70884984  5213.9000851   5235.09132035  5256.28255561\n",
      "  5277.47379087  5298.66502613  5319.85626139  5341.04749665\n",
      "  5362.2387319   5383.42996716  5404.62120242  5425.81243768\n",
      "  5447.00367294  5468.1949082   5489.38614345  5510.57737871\n",
      "  5531.76861397  5552.95984923  5574.15108449  5595.34231975\n",
      "  5616.533555    5637.72479026  5658.91602552  5680.10726078\n",
      "  5701.29849604  5722.4897313   5743.68096656  5764.87220181\n",
      "  5786.06343707  5807.25467233  5828.44590759  5849.63714285\n",
      "  5870.82837811  5892.01961336  5913.21084862  5934.40208388\n",
      "  5955.59331914  5976.7845544   5997.97578966  6019.16702491\n",
      "  6040.35826017  6061.54949543  6082.74073069  6103.93196595\n",
      "  6125.12320121  6146.31443647  6167.50567172  6188.69690698\n",
      "  6209.88814224  6231.0793775   6252.27061276  6273.46184802\n",
      "  6294.65308327  6315.84431853  6337.03555379  6358.22678905\n",
      "  6379.41802431  6400.60925957  6421.80049482  6442.99173008\n",
      "  6464.18296534  6485.3742006   6506.56543586  6527.75667112\n",
      "  6548.94790637  6570.13914163  6591.33037689  6612.52161215\n",
      "  6633.71284741  6654.90408267  6676.09531793  6697.28655318\n",
      "  6718.47778844  6739.6690237   6760.86025896  6782.05149422\n",
      "  6803.24272948  6824.43396473  6845.62519999  6866.81643525\n",
      "  6888.00767051  6909.19890577  6930.39014103  6951.58137628\n",
      "  6972.77261154  6993.9638468   7015.15508206  7036.34631732\n",
      "  7057.53755258  7078.72878784  7099.92002309  7121.11125835\n",
      "  7142.30249361  7163.49372887  7184.68496413  7205.87619939\n",
      "  7227.06743464  7248.2586699   7269.44990516  7290.64114042\n",
      "  7311.83237568  7333.02361094  7354.21484619  7375.40608145\n",
      "  7396.59731671  7417.78855197  7438.97978723  7460.17102249\n",
      "  7481.36225775  7502.553493    7523.74472826  7544.93596352\n",
      "  7566.12719878  7587.31843404  7608.5096693   7629.70090455\n",
      "  7650.89213981  7672.08337507  7693.27461033  7714.46584559\n",
      "  7735.65708085  7756.8483161   7778.03955136  7799.23078662\n",
      "  7820.42202188  7841.61325714  7862.8044924   7883.99572765\n",
      "  7905.18696291  7926.37819817  7947.56943343  7968.76066869\n",
      "  7989.95190395  8011.14313921  8032.33437446  8053.52560972\n",
      "  8074.71684498  8095.90808024  8117.0993155   8138.29055076\n",
      "  8159.48178601  8180.67302127  8201.86425653  8223.05549179\n",
      "  8244.24672705  8265.43796231  8286.62919756  8307.82043282\n",
      "  8329.01166808  8350.20290334  8371.3941386   8392.58537386\n",
      "  8413.77660912  8434.96784437  8456.15907963  8477.35031489\n",
      "  8498.54155015  8519.73278541  8540.92402067  8562.11525592\n",
      "  8583.30649118  8604.49772644  8625.6889617   8646.88019696\n",
      "  8668.07143222  8689.26266747  8710.45390273  8731.64513799\n",
      "  8752.83637325  8774.02760851  8795.21884377  8816.41007903\n",
      "  8837.60131428  8858.79254954  8879.9837848   8901.17502006\n",
      "  8922.36625532  8943.55749058  8964.74872583  8985.93996109\n",
      "  9007.13119635  9028.32243161  9049.51366687  9070.70490213\n",
      "  9091.89613738  9113.08737264  9134.2786079   9155.46984316\n",
      "  9176.66107842  9197.85231368  9219.04354893  9240.23478419\n",
      "  9261.42601945  9282.61725471  9303.80848997  9324.99972523\n",
      "  9346.19096049  9367.38219574  9388.573431    9409.76466626\n",
      "  9430.95590152  9452.14713678  9473.33837204  9494.52960729\n",
      "  9515.72084255  9536.91207781  9558.10331307  9579.29454833\n",
      "  9600.48578359  9621.67701884  9642.8682541   9664.05948936\n",
      "  9685.25072462  9706.44195988  9727.63319514  9748.8244304\n",
      "  9770.01566565  9791.20690091  9812.39813617  9833.58937143\n",
      "  9854.78060669  9875.97184195  9897.1630772   9918.35431246\n",
      "  9939.54554772  9960.73678298  9981.92801824 10003.1192535\n",
      " 10024.31048875 10045.50172401 10066.69295927 10087.88419453\n",
      " 10109.07542979 10130.26666505 10151.45790031 10172.64913556\n",
      " 10193.84037082 10215.03160608 10236.22284134 10257.4140766\n",
      " 10278.60531186 10299.79654711 10320.98778237 10342.17901763\n",
      " 10363.37025289 10384.56148815 10405.75272341 10426.94395866\n",
      " 10448.13519392 10469.32642918 10490.51766444 10511.7088997\n",
      " 10532.90013496 10554.09137021 10575.28260547 10596.47384073\n",
      " 10617.66507599 10638.85631125 10660.04754651 10681.23878177]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "# get the loss function as a MeanSquaredError loss from tf.keras.losses. \r\n",
    "loss_fn = tf.keras.losses.MeanSquaredError();\r\n",
    "\r\n",
    "\r\n",
    "print(\"loss:\");\r\n",
    "print(loss_fn(y_train[0],predictions[0]).numpy());"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loss:\n",
      "38451044.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=1e-5);\r\n",
    "\r\n",
    "model.compile(loss=loss_fn,optimizer=opt,metrics=['accuracy']);\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "model.fit(x_train,y_train,epochs=10,validation_data=(x_test,y_test), batch_size=10);"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "8/8 [==============================] - 26s 3s/step - loss: 521727.3438 - accuracy: 0.0000e+00 - val_loss: 3.6115e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 24s 3s/step - loss: 521727.3438 - accuracy: 0.0000e+00 - val_loss: 3.6114e-06 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "print(model.summary());"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 500)               1004000   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 500)               250500    \n",
      "=================================================================\n",
      "Total params: 2,507,000\n",
      "Trainable params: 2,507,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "predictions = model(x_train);"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "first_graph = predictions[25];\r\n",
    "print(predictions.shape);\r\n",
    "\r\n",
    "x = np.linspace(0,500,500);\r\n",
    "print(x.shape)\r\n",
    "\r\n",
    "pyplot.plot(x, first_graph);\r\n",
    "pyplot.plot(x,y_set[25])"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11244/1000107387.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfirst_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "c10ae7e12ea03818c78a6003d4d50ef0e5e51e430c65513fe14b678a9b488bb3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}