{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "\r\n",
    "\r\n",
    "import tensorflow as tf\r\n",
    "import random\r\n",
    "from tensorflow.keras.models import Sequential\r\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "\r\n",
    "import data_generator\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "# load in the data \r\n",
    "\r\n",
    "full_dataset = np.load(\"ml_dataset.npy\");"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "# split up the data into the x and y halves\r\n",
    "\r\n",
    "x_set = full_dataset[0];\r\n",
    "y_set = full_dataset[1];"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "# split the data into training and testing sets\r\n",
    "\r\n",
    "\r\n",
    "half = int(len(x_set)/2);\r\n",
    "\r\n",
    "print(f'half: {half}');\r\n",
    "x_train = x_set[0:half];\r\n",
    "x_test = x_set[half:];\r\n",
    "\r\n",
    "y_train = y_set[0:half];\r\n",
    "y_test = y_set[half:];\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "half: 50\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "# print some data diagnostics to ensure we correctly obtained data sets\r\n",
    "\r\n",
    "print(f\"x_set shape: {x_set.shape}\");\r\n",
    "print(f\"y_set shape: {y_set.shape}\");\r\n",
    "\r\n",
    "\r\n",
    "training_point_number = x_set.shape[1];\r\n",
    "print(f\"number of points in each graph set: {training_point_number}\");\r\n",
    "\r\n",
    "\r\n",
    "print(\"number of x training data sets:\");\r\n",
    "print(len(x_train[0:]));\r\n",
    "\r\n",
    "\r\n",
    "print(\"number of y training data sets:\");\r\n",
    "print(len(y_train[0:]));\r\n",
    "\r\n",
    "assert(len(x_train) == len(y_train));\r\n",
    "assert(len(x_test) == len(y_test));\r\n",
    "assert(x_set.shape == y_set.shape);\r\n",
    "\r\n",
    "print(f\"\\nNumber of training graph-solution sets: {len(x_train)}\")\r\n",
    "\r\n",
    "\r\n",
    "print(f\"Number of testing graph-solution sets: {len(x_test)}\")\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "print(f\"\\nDataset shape: {x_set.shape}\")\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x_set shape: (100, 500)\n",
      "y_set shape: (100, 500)\n",
      "number of points in each graph set: 500\n",
      "number of x training data sets:\n",
      "50\n",
      "number of y training data sets:\n",
      "50\n",
      "\n",
      "Number of training graph-solution sets: 50\n",
      "Number of testing graph-solution sets: 50\n",
      "\n",
      "Dataset shape: (100, 500)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "# reshape the x data for the LSTM\r\n",
    "\r\n",
    "\r\n",
    "x_train = x_train.reshape(x_train.shape[0],training_point_number,1);\r\n",
    "x_test = x_test.reshape(x_test.shape[0],training_point_number,1);\r\n",
    "\r\n",
    "print(x_train.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(50, 500, 1)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "#build the model by stacking layers; choose optimizer and loss function. Here we set the activation function to a 'relu'\r\n",
    "\r\n",
    "\r\n",
    "model = tf.keras.models.Sequential()\r\n",
    "\r\n",
    "model.add(LSTM(training_point_number));\r\n",
    "model.add(Dense(training_point_number,activation='relu'));\r\n",
    "model.add(Dropout(0.2));\r\n",
    "'''\r\n",
    "model.add(Dense(training_point_number,activation='relu'));\r\n",
    "model.add(Dropout(0.2));\r\n",
    "\r\n",
    "model.add(Dense(training_point_number,activation='relu'));\r\n",
    "model.add(Dropout(0.2));\r\n",
    "model.add(Dense(training_point_number,activation='relu'));\r\n",
    "model.add(Dropout(0.2));\r\n",
    "'''\r\n",
    "\r\n",
    "model.add(Dense(training_point_number,activation='softmax'));\r\n",
    "\r\n",
    "\r\n",
    "print(\"model built\");\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "model built\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "# get the predictions for the second datapoint\r\n",
    "predictions = model(x_train)\r\n",
    "\r\n",
    "# print the raw predictions for that datapoint. Note that these are \"logits\" or \"log-odds\" scores, which is related to the probability, \r\n",
    "# and also is the inverse of the sigmoid function\r\n",
    "print(\"logits prediction: \");\r\n",
    "print(predictions[0]);\r\n",
    "\r\n",
    "# we can use the softmax function to convert these logits to probabilities for each preciction class\r\n",
    "print(\"odds prediction: \");\r\n",
    "print(tf.nn.softmax(predictions).numpy()[0]);\r\n",
    "\r\n",
    "\r\n",
    "print(f'shape: {predictions.shape}');\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "logits prediction: \n",
      "tf.Tensor(\n",
      "[0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002], shape=(500,), dtype=float32)\n",
      "odds prediction: \n",
      "[0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002\n",
      " 0.002 0.002 0.002 0.002 0.002 0.002 0.002 0.002]\n",
      "shape: (50, 500)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "print(y_train[0])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "# get the loss function as a SparseCategoricalCrossentropy loss from tf.keras.losses. \r\n",
    "loss_fn = tf.keras.losses.MeanAbsoluteError();\r\n",
    "\r\n",
    "\r\n",
    "print(\"loss:\");\r\n",
    "print(loss_fn(y_train[0],predictions[0]).numpy());"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loss:\n",
      "inf\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=1e-7,decay=1e-15);\r\n",
    "\r\n",
    "model.compile(loss=loss_fn,optimizer=opt,metrics=['accuracy']);\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "model.fit(x_train,y_train,epochs=2,validation_data=(x_test,y_test), batch_size=10);"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "2/2 [==============================] - 58s 6s/step - loss: inf - accuracy: 0.0600 - val_loss: 3.7170 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 9s 5s/step - loss: inf - accuracy: 0.0600 - val_loss: 3.7170 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 9s 5s/step - loss: inf - accuracy: 0.0600 - val_loss: 3.7170 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - ETA: 0s - loss: inf - accuracy: 0.0600"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "\r\n",
    "print(model.summary());"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 500)               1004000   \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 500)               250500    \n",
      "=================================================================\n",
      "Total params: 1,505,000\n",
      "Trainable params: 1,505,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}