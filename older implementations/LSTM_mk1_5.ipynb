{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "\r\n",
    "#import tensorflow. (We are probably gonna need it!)\r\n",
    "import tensorflow as tf\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "#say hello to confirm working import and printing\r\n",
    "print(\"hello world!\");\r\n",
    "\r\n",
    "\r\n",
    "#load the mnist dataset, a commonly used database of hand-drawn digits. We will use this as our sample training and validation data\r\n",
    "mnist = tf.keras.datasets.mnist\r\n",
    "\r\n",
    "\r\n",
    "#convert the samples from integer values (0 to 255) into floating points (0.0 to 1.0)\r\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "hello world!\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "print(x_train[:1])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[[0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.01176471 0.07058824 0.07058824 0.07058824 0.49411765 0.53333333\n",
      "   0.68627451 0.10196078 0.65098039 1.         0.96862745 0.49803922\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.11764706 0.14117647 0.36862745 0.60392157\n",
      "   0.66666667 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      "   0.88235294 0.6745098  0.99215686 0.94901961 0.76470588 0.25098039\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.19215686 0.93333333 0.99215686 0.99215686 0.99215686\n",
      "   0.99215686 0.99215686 0.99215686 0.99215686 0.99215686 0.98431373\n",
      "   0.36470588 0.32156863 0.32156863 0.21960784 0.15294118 0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.07058824 0.85882353 0.99215686 0.99215686 0.99215686\n",
      "   0.99215686 0.99215686 0.77647059 0.71372549 0.96862745 0.94509804\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.31372549 0.61176471 0.41960784 0.99215686\n",
      "   0.99215686 0.80392157 0.04313725 0.         0.16862745 0.60392157\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.05490196 0.00392157 0.60392157\n",
      "   0.99215686 0.35294118 0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.54509804\n",
      "   0.99215686 0.74509804 0.00784314 0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.04313725\n",
      "   0.74509804 0.99215686 0.2745098  0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.1372549  0.94509804 0.88235294 0.62745098 0.42352941 0.00392157\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.31764706 0.94117647 0.99215686 0.99215686 0.46666667\n",
      "   0.09803922 0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.17647059 0.72941176 0.99215686 0.99215686\n",
      "   0.58823529 0.10588235 0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.0627451  0.36470588 0.98823529\n",
      "   0.99215686 0.73333333 0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.97647059\n",
      "   0.99215686 0.97647059 0.25098039 0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.18039216 0.50980392 0.71764706 0.99215686\n",
      "   0.99215686 0.81176471 0.00784314 0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.15294118 0.58039216 0.89803922 0.99215686 0.99215686 0.99215686\n",
      "   0.98039216 0.71372549 0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.09411765 0.44705882\n",
      "   0.86666667 0.99215686 0.99215686 0.99215686 0.99215686 0.78823529\n",
      "   0.30588235 0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.09019608 0.25882353 0.83529412 0.99215686\n",
      "   0.99215686 0.99215686 0.99215686 0.77647059 0.31764706 0.00784314\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.07058824 0.67058824 0.85882353 0.99215686 0.99215686 0.99215686\n",
      "   0.99215686 0.76470588 0.31372549 0.03529412 0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.21568627 0.6745098\n",
      "   0.88627451 0.99215686 0.99215686 0.99215686 0.99215686 0.95686275\n",
      "   0.52156863 0.04313725 0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.53333333 0.99215686\n",
      "   0.99215686 0.99215686 0.83137255 0.52941176 0.51764706 0.0627451\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "\r\n",
    "#build the model by stacking layers; choose optimizer and loss function. Here we set the activation function to a 'relu'\r\n",
    "model = tf.keras.models.Sequential([\r\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\r\n",
    "  tf.keras.layers.Dropout(0.2),\r\n",
    "  tf.keras.layers.Dense(10)\r\n",
    "])\r\n",
    "\r\n",
    "# get the predictions for the second datapoint\r\n",
    "predictions = model(x_train[:1]).numpy()\r\n",
    "\r\n",
    "# print the raw predictions for that datapoint. Note that these are \"logits\" or \"log-odds\" scores, which is related to the probability, \r\n",
    "# and also is the inverse of the sigmoid function\r\n",
    "print(\"logits prediction: \");\r\n",
    "print(predictions);\r\n",
    "\r\n",
    "# we can use the softmax function to convert these logits to probabilities for each preciction class\r\n",
    "print(\"odds prediction: \");\r\n",
    "print(tf.nn.softmax(predictions).numpy());\r\n",
    "\r\n",
    "\r\n",
    "# since the model is untrained we expect basically random values for each class, which is what we get\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# get the loss function as a SparseCategoricalCrossentropy loss from tf.keras.losses. \r\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True);\r\n",
    "\r\n",
    "\r\n",
    "'''\r\n",
    "from https://www.tensorflow.org/tutorials/quickstart/beginner\r\n",
    "\r\n",
    "\"This loss is equal to the negative log probability of the true class: It is zero if the model is sure of the correct class.\r\n",
    "\r\n",
    "This untrained model gives probabilities close to random (1/10 for each class), so the initial loss should be close to -tf.math.log(1/10) ~= 2.3.\"\r\n",
    "'''\r\n",
    "\r\n",
    "# display loss\r\n",
    "print(\"loss:\");\r\n",
    "print(loss_fn(y_train[:1], predictions).numpy());\r\n",
    "\r\n",
    "# the loss is relatively large, because like we said earlier, the model is untrained\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# compile the model to allow for optimization ( in this case we are using adam ) for increased model training and evaluation efficiency\r\n",
    "model.compile(optimizer='adam',\r\n",
    "              loss=loss_fn,\r\n",
    "              metrics=['accuracy'])\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# train the model over 5 epochs (repetitions)\r\n",
    "model.fit(x_train, y_train, epochs=5)\r\n",
    "\r\n",
    "\r\n",
    "# test the performance of the model ( usually would be over a validation or test set )\r\n",
    "model.evaluate(x_test,  y_test, verbose=2)\r\n",
    "\r\n",
    "# the model is now trained to ~98% accuracy. Pretty good!\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# you can wrap the trained model and attatch a softmax to it if you want it to return probabilities\r\n",
    "probability_model = tf.keras.Sequential([\r\n",
    "  model,\r\n",
    "  tf.keras.layers.Softmax()\r\n",
    "])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "logits prediction: \n",
      "[[-0.7410821  -0.24468224  0.24876298  0.04316191 -0.77958953  0.0912341\n",
      "  -0.03860385  0.49694    -0.00283952 -0.28300315]]\n",
      "odds prediction: \n",
      "[[0.05018556 0.08244466 0.1350402  0.10994403 0.04828978 0.11535838\n",
      "  0.10131207 0.17307924 0.10500102 0.07934508]]\n",
      "loss:\n",
      "2.1597116\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2969 - accuracy: 0.9141\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1432 - accuracy: 0.9576\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1060 - accuracy: 0.9675\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0866 - accuracy: 0.9726\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0737 - accuracy: 0.9774\n",
      "313/313 - 0s - loss: 0.0793 - accuracy: 0.9762\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}